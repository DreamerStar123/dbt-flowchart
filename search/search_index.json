{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dbt-airflow A python package used to render dbt projects via Airflow DAGs. Every dbt resource type, including models, seeds, snapshots and tests, will be assigned an individual task and the dependencies will automatically be inferred. Additional tasks can also be added either before/after the entire dbt project, or in-between specific dbt tasks. Installation The package is available on PyPI : pip install dbt-airflow Usage First, make sure that you have a fresh and up to date manifest.json metadata file that is generated by dbt CLI (hint: See here the list of commands that will generate one). from datetime import datetime from pathlib import Path from airflow import DAG from airflow.operators.dummy import DummyOperator from dbt_airflow.core.task_group import DbtTaskGroup with DAG( dag_id='test_dag', start_date=datetime(2021, 1, 1), catchup=False, tags=['example'], ) as dag: t1 = DummyOperator(task_id='dummy_1') t2 = DummyOperator(task_id='dummy_2') tg = DbtTaskGroup( group_id='dbt-company', dbt_manifest_path=Path('/path/to/target/manifest.json'), dbt_target='dev', dbt_project_path=Path('/path/to/dbt/project/dir'), dbt_profile_path=Path('/path/to/dbt/project/profiles/dir'), ) t1 >> tg >> t2 DbtTaskGroup accepts the following arguments: - group_id (required): The name of the Task Group that will contain dbt project's tasks - dbt_manifest_path (required): The path to the manifest.json file - dbt_target (required): - dbt_project_path (required): - dbt_profile_path (required): - create_sub_task_groups (optional): - extra_tasks (optional): Things to know Here's a list of some key aspects and assumptions of the implementation: Every dbt project, when compiled, will generate a metadata file under <dbt-project-dir>/target/manifest.json The manifest file contains information about the interdependencies of the project's data models dbt-airflow aims to extract these dependencies such that every dbt entity (snapshot, model, test and seed) has its own task in a Airflow DAG while entity dependencies are persisted Snapshots are never an upstream dependency of any task The creation of snapshots on seeds does not make sense, and thus not handled (not even sure if this is even possible on dbt side) Models may have tests Snapshots may have tests Seeds may have tests","title":"Overview"},{"location":"#dbt-airflow","text":"A python package used to render dbt projects via Airflow DAGs. Every dbt resource type, including models, seeds, snapshots and tests, will be assigned an individual task and the dependencies will automatically be inferred. Additional tasks can also be added either before/after the entire dbt project, or in-between specific dbt tasks.","title":"dbt-airflow"},{"location":"#installation","text":"The package is available on PyPI : pip install dbt-airflow","title":"Installation"},{"location":"#usage","text":"First, make sure that you have a fresh and up to date manifest.json metadata file that is generated by dbt CLI (hint: See here the list of commands that will generate one). from datetime import datetime from pathlib import Path from airflow import DAG from airflow.operators.dummy import DummyOperator from dbt_airflow.core.task_group import DbtTaskGroup with DAG( dag_id='test_dag', start_date=datetime(2021, 1, 1), catchup=False, tags=['example'], ) as dag: t1 = DummyOperator(task_id='dummy_1') t2 = DummyOperator(task_id='dummy_2') tg = DbtTaskGroup( group_id='dbt-company', dbt_manifest_path=Path('/path/to/target/manifest.json'), dbt_target='dev', dbt_project_path=Path('/path/to/dbt/project/dir'), dbt_profile_path=Path('/path/to/dbt/project/profiles/dir'), ) t1 >> tg >> t2 DbtTaskGroup accepts the following arguments: - group_id (required): The name of the Task Group that will contain dbt project's tasks - dbt_manifest_path (required): The path to the manifest.json file - dbt_target (required): - dbt_project_path (required): - dbt_profile_path (required): - create_sub_task_groups (optional): - extra_tasks (optional):","title":"Usage"},{"location":"#things-to-know","text":"Here's a list of some key aspects and assumptions of the implementation: Every dbt project, when compiled, will generate a metadata file under <dbt-project-dir>/target/manifest.json The manifest file contains information about the interdependencies of the project's data models dbt-airflow aims to extract these dependencies such that every dbt entity (snapshot, model, test and seed) has its own task in a Airflow DAG while entity dependencies are persisted Snapshots are never an upstream dependency of any task The creation of snapshots on seeds does not make sense, and thus not handled (not even sure if this is even possible on dbt side) Models may have tests Snapshots may have tests Seeds may have tests","title":"Things to know"},{"location":"contributing/","text":"Contributing to dbt-airflow We encourage everyone to be part of this journey and contribute to the project. You can do so by implementing new features, fixing potential bugs or even improving our documentation and CI/CD pipelines. If you are planning to contribute with the implementation of a new feature, you should first open an Issue on GitHub where you can share your ideas and designs so that project's contributors and maintainers can provide their insights. This will ensure that you won't be spending your time on implementing a piece of work that might not support the longer vision of the project and unavoidably might not be merged. Setting up a local environment The next few sections will help you set up a local development environment where you can quickly test your changes, before opening a Pull Request. Creating a fork The first thing you need to do, is to create a fork of the repository. You can do so by clicking on the Fork button that is found on the top right corner of the project's page on GitHub. Once you create a fork, you'll then have to clone your fork onto your local machine. Setup your local environment In your forked project's directory, create and activate a fresh virtual environment: # Create a virtual environment called `dbt-airflow-venv` $ python3 -m venv ~/dbt-airflow-venv # Activate the newly created virtual environment $ source ~/dbt-airflow-venv/bin/activate Every single merge into main branch will trigger a new patch, minor or major version upgrade based on the commit messages pushed from the Pull Request. The automated release mechanism is based on conventional commits . Every single commit must follow the structural elements described in Conventional Commits' specification. The repository also contains pre-commit hooks that will ensure compliance to the specification. Make sure to install pre-commit hooks to avoid any inconsistencies, by following the steps outlined below. # Install `pre-commit` package from PyPI $ python3 -m pip install pre-commit # Install hooks from `.pre-commit-config.yaml` $ pre-commit install pre-commit installed at .git/hooks/pre-commit Testing your changes locally In order to see how your changes will be reflected on Airflow, you can spin up the docker containers from the images specified in docker-compose.yml and Dockerfile files. # Build the images (you can omit `--no-cache` in case you don't want to re-build every layer) $ docker compose build --no-cache # Run the containers $ docker compose up Basically, the commands above will spin up the following containers: - An Airflow instance whose webserver can be accessd on localhost:808 (use airflow and airflow in user/pass credentials) - A postgres instance containing the popular Sakila data, where dbt models can materialize - A container that gives you access to dbt CLI where you can run further dbt commands The Postgres instance can be accessed in the following way (note that default port was changed to 5433 given that we have an additional postgres instance for Airflow itself): # Get the id of the running postgres-sakila container $ docker ps # Enter the running container $ docker exec -it <container-id> /bin/bash # Enter psql $ psql -U postgres -p 5433 You will now be able to run Airflow DAGs authored with the use of dbt-airflow where you can also evaluate results either on the Airflow UI (webserver) or on the local database itself. Additionally, you need to make sure that all tests pass successfully. This project uses poetry tool for dependency management. You'll have to install poetry and install the dependencies specified in pyproject.toml . # Install poetry $ python3 -m pip install poetry==1.3 # Install dependencies $ poetry install If you'd like to run the tests, make sure to do so within the poetry environment, as shown below. # Run all tests $ poetry run pytest tests # Run test(s) with specific prefix or specific name $ poetry run pytest tests -k \"test_some_prefix_or_full_test_name\" Opening a Pull Request Once you have finished your local work, it's time to get it reviewed by project maintainers and other contributors. To do so, create a Pull Request from your fork into the original repository, gmyrianthous/dbt-airflow .","title":"Contributing to dbt-airflow"},{"location":"contributing/#contributing-to-dbt-airflow","text":"We encourage everyone to be part of this journey and contribute to the project. You can do so by implementing new features, fixing potential bugs or even improving our documentation and CI/CD pipelines. If you are planning to contribute with the implementation of a new feature, you should first open an Issue on GitHub where you can share your ideas and designs so that project's contributors and maintainers can provide their insights. This will ensure that you won't be spending your time on implementing a piece of work that might not support the longer vision of the project and unavoidably might not be merged.","title":"Contributing to dbt-airflow"},{"location":"contributing/#setting-up-a-local-environment","text":"The next few sections will help you set up a local development environment where you can quickly test your changes, before opening a Pull Request.","title":"Setting up a local environment"},{"location":"contributing/#creating-a-fork","text":"The first thing you need to do, is to create a fork of the repository. You can do so by clicking on the Fork button that is found on the top right corner of the project's page on GitHub. Once you create a fork, you'll then have to clone your fork onto your local machine.","title":"Creating a fork"},{"location":"contributing/#setup-your-local-environment","text":"In your forked project's directory, create and activate a fresh virtual environment: # Create a virtual environment called `dbt-airflow-venv` $ python3 -m venv ~/dbt-airflow-venv # Activate the newly created virtual environment $ source ~/dbt-airflow-venv/bin/activate Every single merge into main branch will trigger a new patch, minor or major version upgrade based on the commit messages pushed from the Pull Request. The automated release mechanism is based on conventional commits . Every single commit must follow the structural elements described in Conventional Commits' specification. The repository also contains pre-commit hooks that will ensure compliance to the specification. Make sure to install pre-commit hooks to avoid any inconsistencies, by following the steps outlined below. # Install `pre-commit` package from PyPI $ python3 -m pip install pre-commit # Install hooks from `.pre-commit-config.yaml` $ pre-commit install pre-commit installed at .git/hooks/pre-commit","title":"Setup your local environment"},{"location":"contributing/#testing-your-changes-locally","text":"In order to see how your changes will be reflected on Airflow, you can spin up the docker containers from the images specified in docker-compose.yml and Dockerfile files. # Build the images (you can omit `--no-cache` in case you don't want to re-build every layer) $ docker compose build --no-cache # Run the containers $ docker compose up Basically, the commands above will spin up the following containers: - An Airflow instance whose webserver can be accessd on localhost:808 (use airflow and airflow in user/pass credentials) - A postgres instance containing the popular Sakila data, where dbt models can materialize - A container that gives you access to dbt CLI where you can run further dbt commands The Postgres instance can be accessed in the following way (note that default port was changed to 5433 given that we have an additional postgres instance for Airflow itself): # Get the id of the running postgres-sakila container $ docker ps # Enter the running container $ docker exec -it <container-id> /bin/bash # Enter psql $ psql -U postgres -p 5433 You will now be able to run Airflow DAGs authored with the use of dbt-airflow where you can also evaluate results either on the Airflow UI (webserver) or on the local database itself. Additionally, you need to make sure that all tests pass successfully. This project uses poetry tool for dependency management. You'll have to install poetry and install the dependencies specified in pyproject.toml . # Install poetry $ python3 -m pip install poetry==1.3 # Install dependencies $ poetry install If you'd like to run the tests, make sure to do so within the poetry environment, as shown below. # Run all tests $ poetry run pytest tests # Run test(s) with specific prefix or specific name $ poetry run pytest tests -k \"test_some_prefix_or_full_test_name\"","title":"Testing your changes locally"},{"location":"contributing/#opening-a-pull-request","text":"Once you have finished your local work, it's time to get it reviewed by project maintainers and other contributors. To do so, create a Pull Request from your fork into the original repository, gmyrianthous/dbt-airflow .","title":"Opening a Pull Request"},{"location":"examples/","text":"Examples Populating a dbt project on Airflow, as a DAG TODO: Write description from datetime import datetime from pathlib import Path from airflow import DAG from airflow.operators.dummy import DummyOperator from dbt_airflow.core.task_group import DbtTaskGroup with DAG( dag_id='test_dag', start_date=datetime(2021, 1, 1), catchup=False, tags=['example'], ) as dag: t1 = DummyOperator(task_id='dummy_1') t2 = DummyOperator(task_id='dummy_2') tg = DbtTaskGroup( group_id='dbt-company', dbt_manifest_path=Path('/path/to/target/manifest.json'), dbt_target='dev', dbt_project_path=Path('/path/to/dbt/project/dir'), dbt_profile_path=Path('/path/to/dbt/project/profiles/dir'), ) t1 >> tg >> t2 Airflow DAG with dbt project and additional dependencies TODO","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#populating-a-dbt-project-on-airflow-as-a-dag","text":"TODO: Write description from datetime import datetime from pathlib import Path from airflow import DAG from airflow.operators.dummy import DummyOperator from dbt_airflow.core.task_group import DbtTaskGroup with DAG( dag_id='test_dag', start_date=datetime(2021, 1, 1), catchup=False, tags=['example'], ) as dag: t1 = DummyOperator(task_id='dummy_1') t2 = DummyOperator(task_id='dummy_2') tg = DbtTaskGroup( group_id='dbt-company', dbt_manifest_path=Path('/path/to/target/manifest.json'), dbt_target='dev', dbt_project_path=Path('/path/to/dbt/project/dir'), dbt_profile_path=Path('/path/to/dbt/project/profiles/dir'), ) t1 >> tg >> t2","title":"Populating a dbt project on Airflow, as a DAG"},{"location":"examples/#airflow-dag-with-dbt-project-and-additional-dependencies","text":"TODO","title":"Airflow DAG with dbt project and additional dependencies"}]}